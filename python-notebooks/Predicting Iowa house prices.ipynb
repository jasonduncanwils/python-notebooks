{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "e81ee64d-e474-4662-9036-ce23df615199",
        "_uuid": "b6269c0e8f417f82daf093dda8fa0da6d2c57d86"
      },
      "cell_type": "markdown",
      "source": "This is the basis of my submission to the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. This notebook was started based on the lessons on machine learning curated on [Kaggle Learn](https://www.kaggle.com/learn/machine-learning) then refined further to achieve more accuracy and additional visualization.\n\n<br/>\nI also was able to learn some new techniques from the following kernels:\n* [Laurenstc's kernel](https://www.kaggle.com/laurenstc/top-2-of-leaderboard-advanced-fe)\n* [Aleksandrs Gehsbargs's kernel](https://www.kaggle.com/agehsbarg/top-10-0-10943-stacking-mice-and-brutal-force)\n* [Serigne's kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n* [Massquantity's kernel](https://www.kaggle.com/massquantity/all-you-need-is-pca-lb-0-11421-top-4)\n* [Fkstepz's kernel](https://www.kaggle.com/fkstepz/step-by-step-house-prices-prediction)\n\n<br/>\nThe notebook is arranged into the following sections:<br/>\n1. Prep<br/>\n2. Exploratory Visualization and Data Cleansing<br/>\n3. Feature Engineering<br/>\n4. Modeling<br/>\n5. Submit results"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_cell_guid": "f633c6d4-4a7c-4c65-a2c5-2d32e8770c6f",
        "_uuid": "a475ed366d9b2935c233868bd6847dafe382564d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nurl = 'http://ep60qmdjq8-flywheel.netdna-ssl.com/wp-content/uploads/2009/08/commercial-real-estate.jpg'\nImage(url=url,width=800, height=600)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "009915c5-3584-4f5b-b3d2-1fa66dec98c0",
        "_uuid": "d8eaa4c62fc0efe1a4bb55a6eff21dc86fc718d8"
      },
      "cell_type": "markdown",
      "source": "# 1. Prep\nImport all libraries, define functions, and load data."
    },
    {
      "metadata": {
        "_cell_guid": "1b2fee56-4989-4887-9911-1b0c514d94ee",
        "_uuid": "4b67545521f40f58f41bf9ec91cc56f5d0855c4d"
      },
      "cell_type": "markdown",
      "source": "**Import all libraries and define functions**"
    },
    {
      "metadata": {
        "scrolled": false,
        "_cell_guid": "86b26423-563a-4fa1-a595-89e25ff93089",
        "_uuid": "1c728098629e1301643443b1341556a15c089b2b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom xgboost import XGBRegressor\nimport warnings\n\n# Turn off the nagging warnings from sklearn and seaborn\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Identify numeric columns\ndef numeric_cols(data_frame):\n    numeric_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].dtype in ['int64', 'float64']]\n    return(numeric_cols)\n\n# Identify categorical columns with low cardinality (a small number of distinct values)\ndef low_cardinality_cols(data_frame):\n    low_cardinality_cols = [cname for cname in data_frame.columns if \n                                data_frame[cname].nunique() < 50 and\n                                data_frame[cname].dtype == \"object\"]\n    return(low_cardinality_cols)\n\n# Identify columns with missing data\ndef cols_with_missing(data_frame):\n    cols_with_missing = [cname for cname in data_frame.columns \n                                 if data_frame[cname].isnull().any()]\n    return(cols_with_missing)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "58d28dad-9326-443a-88de-6568cc8f2fd2",
        "_uuid": "ef4fce65ddb6d15a05d2f93ca2e86f487dd48a8b"
      },
      "cell_type": "markdown",
      "source": "**Load the data**"
    },
    {
      "metadata": {
        "_cell_guid": "f3401f07-4c4c-4469-aaac-a75bf01672e6",
        "_uuid": "ded85e5015324ca9c284fd09e4a57f4b9a424c14",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Read core training and test data\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\nprint(\"Train set size:\", train_data.shape)\nprint(\"Test set size:\", test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "40044b18-7158-4ac1-b675-5f01be41384c",
        "_uuid": "5ff40589f5ced344bc70fd9e7a06b679d35535b8"
      },
      "cell_type": "markdown",
      "source": "# 2. Exploratory Visualization and Data Cleansing"
    },
    {
      "metadata": {
        "_cell_guid": "1214610b-b8bd-4e51-8c63-e65d805df06d",
        "_uuid": "64c7f047a740cbe185304eb0bd1d1da4d0c716d5"
      },
      "cell_type": "markdown",
      "source": "**Confirm column overlap between train and test**\n\nWhile it can be assumed that all of the columns match in these data sets, I still wanted to get the code in here to confirm that SalePrice is the only column difference between the two."
    },
    {
      "metadata": {
        "_cell_guid": "b1f37d35-1429-4084-829f-d2e0bc7711e4",
        "_uuid": "b3110c7d080ee7fa60cfe54619a934791f6d6d0a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Columns in training data but not in testing data\")\nprint([x for x in train_data.columns if x not in test_data.columns])\nprint(\"Columns in testing data but not in training data\")\nprint([x for x in test_data.columns if x not in train_data.columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "253e53ea-1c75-43f3-85d4-fa9ecdb75319",
        "_uuid": "93f4e3140eeb0415a6f537d224e32e7b20388b0b"
      },
      "cell_type": "markdown",
      "source": "**View statistical properties of the data**"
    },
    {
      "metadata": {
        "_cell_guid": "e6e47dc8-7a22-4249-9c5e-d1bacd455534",
        "_uuid": "8fa9d31ff437378d97e976464c28b694a685b581",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# view the data and get the statistical properties\ntrain_data.describe(include = 'all')\n#train_data.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7f70c82f-fd90-4fe5-b9d3-de5ae046c324",
        "_uuid": "7a609f6baa0c926921865486b6291d8b85b7f54a"
      },
      "cell_type": "markdown",
      "source": "**Drop the Id column since it isn't worth keeping**\n\nBy determining that all values of this column are unique I feel comfortable dropping it"
    },
    {
      "metadata": {
        "_cell_guid": "d65b7997-a743-4f65-9f41-76c4ea8268a4",
        "_uuid": "7ca47d9b9bac7faa5f7741f304f2a320dde20f24",
        "trusted": true
      },
      "cell_type": "code",
      "source": "if train_data['Id'].nunique() == train_data['Id'].size:\n    train_data.drop(['Id'],axis=1,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b7637285-f7cd-4003-90e1-b7f94334b1b1",
        "_uuid": "f92476ef8baec1fb9bdd87ca0ebc5909a40bf4f5"
      },
      "cell_type": "markdown",
      "source": "**View correlation of data**\n\nView the correlation map of data with clean colors and elimination of the duplicate display on the upper right hand corner of the chart that comes with it by default."
    },
    {
      "metadata": {
        "_cell_guid": "16f7f382-a3be-441d-be20-737a9966935d",
        "_uuid": "415e464df99ab71129445904edd881ab77a2b709",
        "trusted": true
      },
      "cell_type": "code",
      "source": "corrmat = train_data.corr()\nplt.subplots(figsize=(12,9))\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ef3f4229-5da1-4b05-b65c-b561fa2ca274",
        "_uuid": "97613378965e9085edbcd091fbabf0a7c0f6b669"
      },
      "cell_type": "markdown",
      "source": "**Get the data into  better working form**"
    },
    {
      "metadata": {
        "_cell_guid": "64ee7c57-44fa-44a8-b610-dbc049e18460",
        "_uuid": "7aaaaab8b89c7b7eda732769258608cc10a81eb0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# pull all data data into target (y) and predictors (X)\ntrain_y = train_data.SalePrice\n\n# only keep data that is either numeric or has a low cardinality for categorical data\n# in this case it keeps most all of the columns\ntargeted_train_X_cols = low_cardinality_cols(train_data) + numeric_cols(train_data)\ntrain_X = train_data[targeted_train_X_cols]\ntrain_X = train_X.drop(['SalePrice'], axis=1)\ntrain_X = pd.get_dummies(train_X)\n\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntargeted_test_X_cols = low_cardinality_cols(test_data) + numeric_cols(test_data)\ntest_X = test_data[targeted_test_X_cols]\ntest_X = pd.get_dummies(test_X)\n\n# inner join the data to ensure the exact columns included are aligned\ntrain_X, test_X = train_X.align(test_X,\n                                join='inner', \n                                axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "97f299a3-5296-4405-89ea-582e10b87eae",
        "_uuid": "37b92cddc30cb10d4586eccfb953fd08b33e8d3b"
      },
      "cell_type": "markdown",
      "source": "**Plot all significant features along with a regression line**"
    },
    {
      "metadata": {
        "_cell_guid": "83cac469-11cd-4659-b86f-c577d96e2930",
        "collapsed": true,
        "_uuid": "4e4c573e038f4364e3c6670ed5b95f106b92a4ca",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Get columns that are most correlated to SalePrice to scatter plot them\ntesting_columns = corrmat.loc[(corrmat.index != 'SalePrice'), (corrmat.SalePrice > 0)].index\n\n# Count subplots, total columns of plot, and total rows of plots\nsubplot_count = int(len(testing_columns))\ncol_count = 2\nrow_count = int((subplot_count/col_count)+1)\n\n# Initiate the subplots\nfig, axes = plt.subplots(nrows=row_count, \n                         ncols=col_count, \n                         sharey=True, \n                         figsize=(12,row_count*2), \n                         squeeze=False)\naxes_list = [item for sublist in axes for item in sublist] \nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=1.5, wspace=0.4)\niter_col_count = 0\niter_row_count = 0\n\n# Loop through all columns to create a subplot for each\nfor i in testing_columns:\n    ax = axes_list.pop(0)\n    ax = sns.regplot(train_X[i],\n                     train_y,\n                     ax=axes[iter_row_count][iter_col_count],\n                     color='blue'\n                    )\n    if iter_col_count == col_count - 1:\n        iter_col_count = 0\n        iter_row_count = iter_row_count + 1\n    else:\n        iter_col_count = iter_col_count + 1\n    ax.set_title(i)\n    ax.tick_params(which='both', bottom='off', left='off',\n                   right='off', top='off'\n                  )\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n# delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "49846191-a439-4110-92e2-b186329b8abb",
        "_uuid": "07543345fa7f7e2c475d3a507c0837732eaec207"
      },
      "cell_type": "markdown",
      "source": "**Plot sales price against Living Area sliced by Overall Quality**"
    },
    {
      "metadata": {
        "scrolled": false,
        "_cell_guid": "67bb68ef-55d4-433d-9207-aee12b2fbe47",
        "_uuid": "766559b868866820d3ebda92e0432abdba9643b5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check out \nexplore_data = train_X.copy()\nexplore_data['SalePrice'] = train_data.SalePrice\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data, fit_reg=False)\n\nsns.lmplot(x='GrLivArea', y='SalePrice', hue='OverallQual',\n           #markers=['o', 'x', '*'], \n           data=explore_data.loc[explore_data['OverallQual'] == 10], fit_reg=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b15b3638-a041-4b33-96fd-b6a2cbb23aab",
        "_uuid": "6e1bcbd8981028881f7dbd9f7f425f33b5ab86e7"
      },
      "cell_type": "markdown",
      "source": "**Run outlier detection on Overall Quality of 10**\n\nFrom the chart above we can clearly see that there are a few outliers for quality 10 so we'll run the Local Outlier Factor to detect these outliers. (anything with a -1 is an outlier)"
    },
    {
      "metadata": {
        "_cell_guid": "cd114148-b9f9-4b2a-8039-9acc8727c7ec",
        "_uuid": "b7958039e2759c0fec3b5a6e6e28b2fe05bb1455",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# detect outliers\noutlier_detect = explore_data.loc[explore_data['OverallQual'] == 10]\noutlier_detect = outlier_detect[['SalePrice','GrLivArea']]\noutlier_detect['outlier'] = LocalOutlierFactor(n_neighbors=20).fit_predict(outlier_detect)\nprint(outlier_detect)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "efb3f34d-0be3-4c18-979f-baca63f61147",
        "_uuid": "dd246a639c087d8246679e69d07d6fe301816a75"
      },
      "cell_type": "markdown",
      "source": "**Remove the outliers from our data set**"
    },
    {
      "metadata": {
        "_cell_guid": "819032eb-2348-4e21-97f4-34ac16a2ac32",
        "_uuid": "17ff2b0952609ca102c3ac4540257ddd32c8f66c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# remove outliers from primary training data sets\noutliers = outlier_detect.loc[outlier_detect['outlier']==-1].index.values\nprint('Shape of data prior to removal')\nprint(train_X.shape)\nprint(train_y.shape)\ntrain_X.drop(outliers, inplace=True)\ntrain_y.drop(outliers, inplace=True)\nprint('Shape of data after removal')\nprint(train_X.shape)\nprint(train_y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "297ceee9-098c-48a3-ac15-af0f3f773bc2",
        "_uuid": "946dd5d784b98a58caaa14bca6cdc470a3f75901"
      },
      "cell_type": "markdown",
      "source": "**Check for skew in the sales price vs transform with log**"
    },
    {
      "metadata": {
        "_cell_guid": "4eee5221-2a15-4749-b84b-6efb8fb6f7ad",
        "_uuid": "ef277294b2e310311e047757805aa25996dd0ab3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.subplots(figsize=(12,9))\nplt.subplot(1, 2, 1)\nsns.distplot(train_y)\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(train_y))\nplt.xlabel('Log SalePrice')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "457dd469-4bc6-403e-8483-5aeaff97ad85",
        "collapsed": true,
        "_uuid": "f1b94ead9a41fe8e7b7fcfc3b3866a0b021c22ac",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# perform log transformation to reduce skew\n#train_y = np.log1p(train_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0c882fed-daa5-4d2a-ac8b-13da13262612",
        "_uuid": "ad79e6f554b292c3e4f3e874da2868d17c436b50"
      },
      "cell_type": "markdown",
      "source": "# 3. Feature engineering"
    },
    {
      "metadata": {
        "_cell_guid": "54f90dea-6ce0-4c4f-8c00-6462590958d3",
        "_uuid": "d6332dcb6fbda7a2862e2fcbce8904946921edc6"
      },
      "cell_type": "markdown",
      "source": "**Sum multiple features together to get overall totals**"
    },
    {
      "metadata": {
        "_cell_guid": "f968c403-77ba-476f-a412-7a2b2ef09ada",
        "_uuid": "f4e7dd0cf28d74c919d535d6241fbe1576b2e75a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add Total SF\nfeatures_to_sum = ['TotalBsmtSF','1stFlrSF','2ndFlrSF']\ntrain_X['newTotalSF'] = train_X[features_to_sum].sum(axis=1)\ntest_X['newTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Finished SF\n#features_to_sum = ['BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF']\n#train_X['newFinTotalSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newFinTotalSF'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Bathrooms\n#features_to_sum = ['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']\n#train_X['newTotalBath'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalBath'] = test_X[features_to_sum].sum(axis=1)\n\n# Add Total Porch SF\n#features_to_sum = ['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF']\n#train_X['newTotalPorchSF'] = train_X[features_to_sum].sum(axis=1)\n#test_X['newTotalPorchSF'] = test_X[features_to_sum].sum(axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "202c7ffa-1111-4b13-97dc-8119fb701250",
        "_uuid": "bec92d903e3f32b29d63655276f91e2e4bef7228"
      },
      "cell_type": "markdown",
      "source": "# 4. Modeling\nBuild best model and predict final results"
    },
    {
      "metadata": {
        "_cell_guid": "783c1c18-6c6b-4e7e-9b21-0e9f74af4091",
        "_uuid": "7d701c9678ebe63334b94c46fade40c29d23242e"
      },
      "cell_type": "markdown",
      "source": "**Create pipeline with models**"
    },
    {
      "metadata": {
        "_cell_guid": "231078a8-1940-45af-bd4a-5efba91c5732",
        "_uuid": "80f5ca3117417ee60941ab3d408688817ed826ca",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create pipeline\nmy_pipeline = make_pipeline(Imputer(),\n                            XGBRegressor(n_estimators=1000, \n                                         learning_rate=0.05)\n                           )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d3204ddb-e70a-48c0-8a0d-e8f9433170fe",
        "_uuid": "8be1f1708a3e709e236bf8291fdffd0300defd53"
      },
      "cell_type": "markdown",
      "source": "**Score pipeline**"
    },
    {
      "metadata": {
        "_cell_guid": "fd41a07a-7a35-4315-9351-3897a31b1c87",
        "_uuid": "7df2855ed1eb4e0ae183b616a9720a1176df69ee",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Scoring - Root Mean Squared Error\ndef rmse_score(model,X,y):\n    return np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n\n# Scoring - Mean Absolute Error\ndef mae_score(model,X,y):\n    return -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=5)\n\n# score the pipeline\n#mae = mae_score(my_pipeline,train_X,train_y)\n#print('Mean Absolute Error: {}'.format(mae.mean()))\n#rmse = rmse_score(my_pipeline,train_X,train_y)\n#print('Root Mean Squared Error: {}'.format(rmse.mean()))\nrmse_log = rmse_score(my_pipeline,train_X,np.log1p(train_y))\nprint('Root Mean Squared Error with log tranform: {}'.format(rmse_log.mean()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ffa78778-5262-4309-be2a-407ecb3d5a90",
        "_uuid": "f80fc6cc072d1891ea1149f1bbc7627d4a47009a"
      },
      "cell_type": "markdown",
      "source": "**Train the model**"
    },
    {
      "metadata": {
        "_cell_guid": "88fd3206-0c3d-4385-8a32-64351b57c79a",
        "_uuid": "65e89dd098bcd438c08c7a3fc5d8788688ff2238",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# train the model\nmodel = my_pipeline.fit(train_X, train_y)\n\n# Use the model to make predictions\nfinal_predicted_prices = model.predict(test_X)\n\n# transform log values for sale price back into regular sale price\n#final_predicted_prices = np.expm1(final_predicted_prices)\n\n# round final prices\nfinal_predicted_prices = [round(x,2) for x in final_predicted_prices]\n\n# look at the predicted prices to ensure we have something sensible.\nprint(final_predicted_prices[:5])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ad6aff50bf2d386bf0b58bf06e6f78eb8cff028"
      },
      "cell_type": "markdown",
      "source": "**Permutation Importance - Importance of various features**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed0bb9ee39fd15b5e20850abd2ed16fcfdb7d0f4"
      },
      "cell_type": "code",
      "source": "#TO DO - if I want this to work it seems I'll need to concentrate on only numeric values\n\n#import eli5\n#from eli5.sklearn import PermutationImportance\n\n# I'm using training data here for now since I didn't take the time to split out validation data \n#perm = PermutationImportance(model, random_state=1).fit(train_X, train_y)\n\n#eli5.show_weights(perm, feature_names = train_X.columns.tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6b5cf662bf4b038359f125e859f81a02ac59256"
      },
      "cell_type": "markdown",
      "source": "**Partial Dependence Plot (PDP)**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f896798b7e493eca31757b613c83690296c7082c"
      },
      "cell_type": "code",
      "source": "from pdpbox import pdp, get_dataset, info_plots\n\ncompare_cols = [cname for cname in test_X.columns]\n\nfeatures_to_review = ['OverallQual','YearBuilt','MoSold','Fireplaces']\n\nfor feat_name in features_to_review:\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=test_X, model_features=compare_cols, feature=feat_name, num_grid_points=100)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()\n\n#help(pdp)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25458f4930e4136c774a620af4cc57051d65371"
      },
      "cell_type": "code",
      "source": "features_to_review2 = ['OverallQual','YearBuilt']\n\npdp_inter1 = pdp.pdp_interact(model=model, dataset=test_X, model_features=compare_cols, features=features_to_review2)\npdp.pdp_interact_plot(pdp_interact_out=pdp_inter1, feature_names=features_to_review2, plot_type='contour')\nplt.show()    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1266d2682d733f95c76bbc10964626e8ced05ffc"
      },
      "cell_type": "markdown",
      "source": "**SHAP Values for one house**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23ae7f5f5e90ad4a02afb05837de2860642a85a2"
      },
      "cell_type": "code",
      "source": "import shap  # package used to calculate Shap values\n\ndef prediction_factors(model, predict_data, core_data):\n    # Had to use KernelExplainer instead of TreeExplainer due to pipeline\n    explainer = shap.KernelExplainer(model, core_data)\n    shap_values = explainer.shap_values(predict_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], predict_data)\n\nsample_data_for_prediction = test_X.iloc[0]\nprediction_factors(my_pipeline, sample_data_for_prediction, test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "09e5bfde-db24-4432-bd48-24298ebb0f05",
        "_uuid": "63a5064af463adb14380fb8c4eed4a14e9433e02"
      },
      "cell_type": "markdown",
      "source": "# 5. Submit results"
    },
    {
      "metadata": {
        "_cell_guid": "2f5d4f77-e594-4cd2-a9e3-c8b1d1cc9152",
        "collapsed": true,
        "_uuid": "dceebe60bfa69f1926c18881c7e57715b9d33535",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# submit results\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': final_predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}